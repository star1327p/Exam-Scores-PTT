To measure the model performance in binary classification, we should evaluate the model's capability to distinguish between positive and negative datapoints.^[<https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5>] For the model validation in Section \ref{cmp-results}, we have to choose a probability threshold in advance to convert the prediction results into the percentages. A predicted probability value is assigned a 1 (positive class) if it is above the threshold, and assigned to 0 (negative class) otherwise. Setting a low threshold allows us to get more predicted positive datapoints, but this increases the risk of getting false positives. On the contrary, setting a high threshold ensures that the predicted positive datapoints are more likely to be actually positive. But the downside is that many actual positive datapoints may not get classified as positives due to the high threshold.   

Therefore, we introduce the ROC (receiver operating characteristic) as a model performance metric that summarizes over all possible probability thresholds.^[<https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb>] The ROC plots the FPR (false positive rate) against the TPR (true positive rate), and the aggregate measure is called AUC (area under the curve).^[<https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc>] AUC is between 0 and 1 like a probability. A model that is 100% correct has an AUC of 1, and a model that is 0% correct has an AUC of 0. However, if we know that a model is going to be 0% correct, we can infer that the actual datapoints are the complete opposite of the model's prediction. This gives us the same level of information as if the model had been 100% correct.^[<https://victorzhou.com/blog/information-gain/>] The baseline should be a completely random guess, where we get 50% correct like a coin flip. Hence the baseline AUC is 0.5.   

According to \citet{yang2017receiver}, the AUC values can be interpreted as below:    
  
  - AUC < 0.5: Performance worse than random guess.

- AUC = 0.5: Completely random guess, i.e., no classification power at all.

- 0.5 < AUC < 0.6: Unacceptable performance.

- 0.6 $\leq$ AUC < 0.7: Minimally acceptable performance.

- 0.7 $\leq$ AUC < 0.8: Adequate performance. 

- 0.8 $\leq$ AUC < 0.9: Great performance.

- AUC $\geq$ 0.9: Excellent performance.

In non-clinical data science, AUC $\geq$ 0.7 is typically required for a model to be effective \cite{han2022utility}. For extremely difficult tasks, 0.6 $\leq$ AUC < 0.7 may be tolerated \cite{kanter2015deep}. If the AUC is close to or less than 0.5, the model has little-to-no power in binary classification because the results are similar to a random guess.  

The `R` package `verification` \cite{r-auc-verification} is a tool to generate the ROC plot and calculate the AUC. The package was originally created to verify weather forecast data, and that's how it got the name `verification`. We will start with a demonstrative ROC curve in Section \ref{roc-demo} using simulated data. Then in Section \ref{roc-prep}, we explore the data and model results, share our observations, and prepare the input for the ROC curve. Finally, we plot the ROC curve from our data and find the AUC in Section \ref{roc-auc-results}.     