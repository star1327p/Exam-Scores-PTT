We decided to use the leave-one-out results to plot the ROC-AUC, rather than the results from K-fold cross-validation or separate training and testing datasets. In the outcomes of separate training and testing datasets (Section \ref{sep-train-test}), only the testing dataset contains predictive probabilities, and we do not think the sample size is large enough for the overall evaluation of model performance. In comparison, both leave-one-out and K-fold cross-validation predict every single record in the data. The main difference is that in leave-one-out (Section \ref{leave-one-out}), each datapoint is predicted by a different training sample. When two datapoints have the same \textbf{HighSchool$\_$PR} but differ in \textbf{College$\_$Score}, they will have different predictive probabilities because the associated training samples are not the same. On the other hand, K-fold (Section \ref{k-fold}) has only $K$ different subsets, so two datapoints in the same subset with the same \textbf{HighSchool$\_$PR} will get the same predicted probability for \textbf{College$\_$Score}. This can result in many repetitive predicted values, which is impractical in real life.   


```{r header-code-test,include=FALSE}
data = read.csv("../ptt_SENIORHIGH_data.csv")
names(data)[1] = "pttID"

missing_rows = which(data$HighSchool_PR == "-1" | data$College_Score == "-1")
data_corr = data[-missing_rows,]

data_corr$CS_65up = data_corr$College_Score >=65

# model = glm(CS_65up ~ HighSchool_PR, data=data_corr, family="binomial")

# Leave-one-out
nn = nrow(data_corr) # total 188 rows of data

prob_leave1out = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_corr[ii,] # reserve one record for testing
  data_exclude = data_corr[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_PR, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out[ii] = test_leave1out
}

print("This is a test for leave-one-out.")
```

### Original Data {#original-roc}

In the implementation of ROC-AUC curve, we use the leave-one-out cross validation output because it predicts every single record in the data. In the first `roc.plot` input, we use `plot=NULL` to avoid creating a plot prematurely. From the `roc.plot` object `roc_data_test`, we can retrieve the AUC value, which is approximately 0.79. This AUC means our model has good performance, according to the guidelines at the beginning of Chapter \ref{roc-auc}.   

```{r roc-auc-with-data}
library(verification)
set.seed(1234)

real_outcomes = data_corr$College_Score >=65
pred_outcomes = as.numeric(prob_leave1out)

roc_data_test = roc.plot(real_outcomes, pred_outcomes, 
                         xlab = "FPR", ylab="TPR", show.thres = FALSE,
                         plot=NULL)

auc_value_test = roc_data_test$roc.vol$Area
print(auc_value_test)
```

Now we add the AUC value to the title, and finally plot the ROC graph from our data. However, this ROC curve contains "steps" and is not smooth, because our model made predictions from discrete data with a relatively small sample size.^[<https://forum.posit.co/t/why-do-smooth-roc-curves/121753>] Each student with the same **HighSchool_PR** should have the same predictive probability of getting **College_Score** at least 65, and the repetitive predictive probablities resulted in the discontinuous steps in the graph.    

Hence we need to smooth the ROC curve. The `R` package `pROC` \cite{r-p-roc} contains many methods to smooth the ROC curve. But `pROC` generates the ROC curve based on specificity and sensitivity, rather than FPR and TPR. If we still use the `R` package `verification`, an easier solution is to manually add a small random noise to **HighSchool_PR** in the data, i.e., jittering. In this way, each subject's **HighSchool_PR** becomes slightly different than each other, so that we can get distinct values.   

```{r plot-roc-auc-with-data}
roc.plot(real_outcomes, pred_outcomes, 
         xlab = "FPR", ylab="TPR",  show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value_test,2), "(Original Data)"))
```

### Jittered Data {#jittered-roc}

Jittering is the process of adding some noise to the numbers, and we can use the `R` function `jitter` perform this modification. The input parameter `factor` controls the variance of the noise. For example, we jitter the numbers 1, 1, 2, 2, 3, 3. Using `factor=2` gives a larger amount of noise than using `factor=1`. Observe that jittering the same value twice generates separate versions of random noise, so the two output numbers are slightly different. We should be careful not to add too much noise, otherwise the original data signal will be buried by the noise. We also need to specify the random seed to ensure reproducibility.   

```{r jitter-example-1}
example_vector = c(1,1,2,2,3,3)
set.seed(20)
jitter(example_vector, factor=1)
```

```{r jitter-example-2}
example_vector = c(1,1,2,2,3,3)
set.seed(20)
jitter(example_vector, factor=2)
```

Before we start jittering the data, we also need to augment the data because we only have 188 valid records in the sample. Data augmentation is the process of artificially generating new data from the original data, mainly for machine learning training purposes.^[<https://aws.amazon.com/what-is/data-augmentation/>] Therefore, we duplicate the 188-record dataset by 20 times, resulting in 3760 total rows. Then we jitter all 3760 datapoints of **HighSchool_PR**, using `factor=2` for the level of noise. An earlier example showed that jittering the same number can produce different outputs, so we can rest assured that each jittered value of **HighSchool_PR** is distinct. In the code, we can see that the first two datapoints are approximately 60.3 and 60.2, indicating that both jittered values had an original **HighSchool_PR** of 60. Parameters are empirically determined here.   

```{r jitter-data-augment}
# Augment the dataset to 188*20=3760 rows.
augment_factor = 20
data_jitter = data_corr
for (aa in 2:augment_factor) {
  data_jitter = rbind(data_jitter, data_corr)
}

# Use a larger variance for the jitter.
set.seed(20)
data_jitter$HighSchool_jitter = jitter(data_jitter$HighSchool_PR, factor=2)

# Show the first 10 jittered values
print(data_jitter$HighSchool_jitter[1:10])
```

**Remark**: `R` uses pass by value in data assignment, so the whole content in `data_corr` is copied to `data_jitter`. Changing the data in `data_jitter` won't change the original values in `data_corr`, which is good. Compared with the pandas DataFrame in Python, we have to explicitly use the function `copy` to make a deep copy of the data. If we simply use assignment, the new DataFrame is simply a pointer to the original DataFrame. Then editing the new dataset will result in changing the original data.^[<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html>]   

Now let's perform the leave-one-out cross validation on the jittered version of data, which contains 3760 total rows. The approach is exactly the same as described in Section \ref{leave-one-out}. We reserve one record for testing, and use all the other records to train the model. Then the model predicts the one datapoint which was left out before. Given the size of the data, it may take more than a few seconds to complete this part and obtain the predictive probabilities.    

```{r jitter-leave1out}
# Leave-one-out for the jittered version of data

nn = nrow(data_jitter) 
# 188*20=3760 total rows of data

prob_leave1out_jitter = rep(c(-1), nn)

for (ii in 1:nn) {
  data_test = data_jitter[ii,] # reserve one record for testing
  data_exclude = data_jitter[-ii,]

  train_leave1out = glm(CS_65up ~ HighSchool_jitter, data=data_exclude, family="binomial")
  # summary(train_leave1out)
  
  test_leave1out = predict.glm(train_leave1out, data_test, type="response")
  # type="response" gives the predicted probabilities

  # Store the predicted probability to the general list
  prob_leave1out_jitter[ii] = test_leave1out
}
```

In `roc.plot`, we need to specify the probability thresholds. Otherwise, the default in `roc.plot` is to use all unique thresholds for each prediction, and there will be way too many unique probability thresholds given a large dataset. We will get the warning message: "Warning: Large amount of unique predictions used as thresholds. Consider specifying thresholds."   

Hence we specify the thresholds as 0.001, 0.002, ..., all the way to 0.999 and 1.000. There are 1000 thresholds in increments of 0.001 within the interval (0,1], so we can create this numerical vector using `c(1:1000)/1000`. The number 1000 is called the `granularity`.^[<https://www.talon.one/glossary/granularity>]   

Although the AUC is slightly higher using the jittered data (81%) than the original data, the small difference is due to the random noise from jittering. If we select another random seed, the resulting AUC would also be close to 80%.  


```{r jitter-roc-auc-calc}
library(verification)
set.seed(1234)

real_outcomes_jitter = data_jitter$College_Score >=65
pred_outcomes_jitter = as.numeric(prob_leave1out_jitter)

# Probability thresholds: 0.001, 0.002, ..., 0.999, 1.000.
granularity = 1000
thresholds = c(1:granularity)/granularity

roc_data_jitter = roc.plot(real_outcomes_jitter, pred_outcomes_jitter, 
                           thresholds = thresholds,
                           xlab = "FPR", ylab="TPR", show.thres = FALSE,
                           plot=NULL)

auc_value_jitter = roc_data_jitter$roc.vol$Area

print(roc_data_jitter$roc.vol$Area)
```

Now the ROC graph is smoother than the original one in Section \ref{original-roc}.  

```{r jitter-roc-auc-plot}
roc.plot(real_outcomes_jitter, pred_outcomes_jitter, 
         thresholds = thresholds,
         xlab = "FPR", ylab="TPR", show.thres = FALSE, 
         main=paste("ROC Curve: AUC =", round(auc_value_jitter,2), "(Jittered Data)"))
```

