Before we perform any statistical modeling, we need to observe the data and this step is called exploratory data analysis. The exploratory phase allows us to identify patterns, detect anomalies, and verify assumptions in the data.^[<https://blog.eduonix.com/bigdata-and-hadoop/importance-exploratory-data-analysis-ml-modelling>] This is an important but often overlooked step in data analysis, and failure to explore the data can lead to inefficiencies such as accurate models on the wrong data.^[<https://bit.ly/3QGGZ60>]   

Therefore, we start with examining the trends of the two main variables, **HighSchool_PR** and **College_Score**. For each variable, we observe the values and summarize them in a histogram as part of the univariate analysis. Then we investigate the relationship between the two variables, i.e., bivariate exploration.  

## High School Entrance Exam Scores (Percentile Rank) {#EDA-HighSchool_PR}

We show the descriptive statistics of **HighSchool_PR** in our data, i.e., the percentile rank of high school entrance exam scores. The invalid or missing values are removed beforehand. The `summary` function returns the minimum, 1st quartile, median, mean, 3rd quartile, and maximum from the input data.   

- **Minimum**: The smallest value in the data.
- **1st Quartile**: The value where 25% of the data is below this point; that is, the middle value between the minimum and the median.
- **Median**: The value where 50% of the data is below this point, and 50% of the data is above this point.
- **Mean**: The arithmetic mean, which is calculated as the sum of the data divided by the number of the points.
- **3rd Quartile**: The value where 75% of the data is below this point; that is, the middle value between the median and the maximum.
- **Maximum**: The largest value in the data.

```{r high-school-pr}
summary(uni_HS_score)
```


**Remark**: Note that we used the full univariate set of **HighSchool_PR** available, rather than taking only those with a valid **College_Score**. The reason is that we wanted to keep as many records as possible. We can do a quick sensitivity analysis here, and the summary statistics are very similar after we exclude the few records without a valid **College_Score**.   

```{r high-school-pr-bivariate}
summary(data_corr$HighSchool_PR)
```

In the univariate analysis, the 1st quartile of **HighSchool_PR** is 85, which means only 25% of the respondents have a percentile rank (PR) below 85. Hence 75% of the respondents have a percentile rank (PR) at least 85, i.e., in the top 15% of the high school entrance exam. The median (92) is higher than the mean (89-90), indicating that the distribution is left-skewed. Half of the respondents scored **HighSchool_PR** 92 or better, and we have few samples of the lower end of the high school entrance exam. In fact, the lowest score we obtained in the data is already 51 -- still slightly above the national median score (50). The maximum is capped at 99.    

The histogram of **HighSchool_PR** in Figure \ref{fig:high-school-pr-histogram} shows a left-skewed distribution, and we can see a long tail to the left. Since our data are self-reported by respondents, the ones with higher scores are more likely to report, resulting in survey non-response bias \cite{elston2021participation}. It is also possible that some students with extremely low **HighSchool_PR** chose not to attend college, so they would not have a **College_Score** to respond to the survey.  

```{r high-school-pr-histogram, fig.cap="Histogram of \\textbf{HighSchool$\\_$PR} (max possible is 99)"}
hist(uni_HS_score, main = "Histogram of HighSchool_PR", 
     xlab="HighSchool_PR (max possible is 99)")
```

If we were to take a sufficiently large random sample from the full database of **HighSchool_PR** in Taiwan, the minimum should be 1 and the maximum would still be at 99. However, the **HighSchool_PR** is the national percentile rank itself, so the 1st quartile should be around 25, median and mean both around 50, and the 3rd quartile around 75.   

As a baseline, we also simulate this random sample by generating data from a **discrete** uniform distribution between 1 and 99. We manually take a random sample of the same size as the univariate **HighSchool_PR** from the integers 1, 2, ..., 99, and we sample with replacement to allow duplicate values. Note that the function `runif`\footnote{The function name \texttt{runif} is pronounced as \texttt{r-unif}, not \texttt{run-if}.} in the `stats` package generates samples from the **continuous** uniform distribution, and that's why we did not use `runif` here.     

```{r gen-random-data}
possible_values = c(1:99)

set.seed(67)
# sample with replacement
random_sample = sample(possible_values, size=length(uni_HS_score), replace=TRUE)
print(random_sample)
```

As we can see, the simulated random sample has median and mean close to 50. The 1st quartile is near 25, and the 3rd quartile is near 75. The simulated sample is to demonstrate a nationally representative sample of **HighSchool_PR**. Nevertheless, this would not be appropriate for the evaluation of the relationship between **HighSchool_PR** and **College_Score**, because students with a higher **HighSchool_PR** tend to be more interested in attending college \cite{yu2018stratification}.   

```{r random-data-stats}
summary(random_sample)
```

Figure \ref{fig:random-data-histogram} shows that the histogram of the simulated random sample is flat, as what we expect from a discrete uniform distribution.  

```{r random-data-histogram, fig.cap="Histogram of Simulated Random Sample for  \\textbf{HighSchool$\\_$PR} (max possible is 99)"}
hist(random_sample, main = "Histogram of Simulated Random Sample", 
     xlab="HighSchool_PR (max possible is 99)")
```

## College Entrance Exam Scores  {#EDA-College_Score}

Similarly, we also show the descriptive statistics of **College_Score**, i.e., the college entrance exam scores between 1 and 75. The distribution is also left-skewed, but less extreme than **HighSchool_PR**. The median of **College_Score** is 64.5, indicating that 50% of the respondents have **College_Score** 65 or higher. The 3rd quartile is already at 69, so the top score range 69-75 accounts for 25% of the respondents. This is much higher than the national average.  

```{r college-score}
summary(uni_college_score)
```

As a sensitivity analysis check, we also examine the `summary` of the **College_Score** datapoints only when they have a corresponding valid **HighSchool_PR**. The distribution is almost the same.  

```{r college-score-bivariate}
summary(data_corr$College_Score)
```

According to the reference score table^[<https://bit.ly/3bAYOvO>] from Wikipedia, the 88th percentile of the college entrance score fluctuates around 60 in Years 2004-2010, and 62-65 in Years 2011-2018. Since the median of **College_Score** is 64.5, we can infer that at least 50% of the respondents scored in the top 12% of the college entrance exam. On the other hand, the reference score table also shows that the 75th percentile of the college entrance score is between 53 and 58 in Years 2004-2018. The PTT data's 1st quantile is already at 58, so we can also infer that at least 75% of the respondents scored in the top 25% of the college entrance exam.   

Since PTT contains forums for several prestigious universities in Taiwan, it is no surprise that many users attended these colleges because they scored well on the college entrance exam. Nevertheless, PTT does not limit registration to students of these colleges, so the population of PTT is relatively diverse but still not very representative of the whole student population.   

The histogram for **College_Score** in Figure \ref{fig:college-score-histogram} also shows a left-skewed distribution, but not as extreme as **HighSchool_PR**. Note that right-skewed distributions are more common in real life, such as employee salaries and movie ticket sales.^[<https://www.statology.org/positively-skewed-distribution-examples/>]       
```{r college-score-histogram, fig.cap="Histogram of \\textbf{College$\\_$Score} (max possible is 75)"}
hist(uni_college_score, main="Histogram of College_Score",
     xlab="College_Score (max possible is 75)", xlim=c(30,80))
```

Unlike the case of **HighSchool_PR**, we do not think it is appropriate to simulate a random sample for **College_Score** across years in general. First, **College_Score** are not percentile ranks like **HighSchool_PR**, and there is no statistical nature of how the **College_Score** distribution should look like. Moreover, the exact distribution of **College_Score** differs greatly due to the varying difficulty levels each year. For example, the 88th percentile of **College_Score** was 65 in 2014 and 59 in 2007.^[<https://bit.ly/2W0fdUq>] The 6-point difference in the score is large enough for an applicant to get admitted to a lower or higher tier of universities.   

In Section \ref{College_Score}, we mentioned that the Taiwan government published detailed statistics of the **College_Score** each year, including the five indicators (88th, 75th, 50th, 25th, 12th percentile) of each subject and the total score. However, since our data sample contains **College_Score** across several years, it is not meaningful to generate a simulated distribution here.   

## Bivariate Exploration {#bivariate}

Next, we create a bivariate scatterplot of **HighSchool_PR** and **College_Score** in Figure \ref{fig:bivariate} to examine the relationship between the two scores. Obviously, a respondent needs a valid score in both to be included in the bivariate scatterplot. Just like what we observed in the univariate plots, both variables are largely concentrated towards the maximum possible scores. The bivariate scatterplot shows a funnel shape -- respondents with higher **HighSchool_PR** have larger variance in **College_Score**.      

```{r bivariate, fig.cap="Bivariate scatterplot of \\textbf{HighSchool$\\_$PR} and  \\textbf{College$\\_$Score}"} 
plot(data_corr$HighSchool_PR, data_corr$College_Score,
     main="High School and College Entrance Exam Scores",
     xlab="HighSchool_PR", ylab="College_Score")
```

### Correlation Coefficient

We use the correlation coefficient to measure the strength of the linear relationship between **HighSchool_PR** and **College_Score**. The computed value is approximately 0.507, showing a medium strength of positive association between the two variables. We can interpret that a better score in the high school entrance exam is associated with a better college entrance exam score, but the relationship is not as strong after **HighSchool_PR** reaches 80. Note that correlation does not imply causality \cite{ksir2016correlation, schield1995correlation}.     

```{r correlation}
cor(data_corr$HighSchool_PR, data_corr$College_Score)
```

To find the correlation coefficient between the random variables $X, Y$, we start with the covariance $\text{Cov}(X,Y)$ in the equation below. $E[X]$ denotes the expectation value of $X$, a.k.a. the mean of $X$.

$$\text{Cov}(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y].$$

Then we also need to compute the standard deviation $\sigma_X$:

$$\sigma_X = \sqrt{E[(X-E[X])^2]} = \sqrt{E[X^2]-(E[X])^2}.$$
Similarly, the standard deviation $\sigma_Y$ is:  

$$\sigma_Y = \sqrt{E[(Y-E[Y])^2]} = \sqrt{E[Y^2]-(E[Y])^2}.$$

Finally, we can calculate the correlation coefficient as:

$$\rho_{X,Y} = \dfrac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}.$$


The correlation coefficient $\rho$ is always between -1 and +1. The stronger $|\rho_{X,Y}|$ is, the stronger is the linear relationship. A positive value means the two variables $X,Y$ are positively associated with each other. In other words, when $X$ increases, $Y$ is also expected to increase. A negative value means the two variables $X,Y$ are negatively associated with each other. In this case, when $X$ increases, $Y$ is expected to decrease. The correlation coefficient may be zero, but this simply means that $X$ and $Y$ do not have a linear relationship with each other. There may exist other patterns between the two variables.  

According to \citet{rumsey2016interpret}, the strength of linear relationship can be interpreted as below:     

- $|\rho| = 1$: Perfect linear relationship

- $0.6 \leq |\rho| < 1$: Strong linear relationship

- $0.4 \leq |\rho| < 0.6$: Moderate linear relationship

- $0.2 \leq |\rho| < 0.4$: Low linear relationship

- $0 < |\rho| < 0.2$: Little-to-no linear relationship

- $|\rho| = 0$: No linear relationship


### Examples of Scatterplots

Here we visualize some bivariate patterns for different values of the correlation coefficient $\rho$, with hypothetical data and examples. The graph ideas came from *OpenIntro Statistics* \cite{diez2019openintro}. We hide the code that generated these graphs to avoid obscuring the topic, and the interested readers can find the code in the raw `.Rmd` files. We will discuss more about linear regression in Chapter \ref{linear-reg}.     


```{r correlation-graph-function, echo=FALSE}
# Graph ideas from *OpenIntro Statistics*, especially pp.311 (Section 8.1).  
# https://github.com/OpenIntroStat/openintro-statistics/tree/master/ch_regr_simple_linear   

linear_corr_gen <- function(noise_sd, relationship="positive", seed=1000){
  
  x = c(-100:100)/20

  if (noise_sd < 0) {
    noise_sd = abs(noise_sd)
    print("Warning: Standard deviation should be a positive value.")
  }
  
  if (noise_sd == 0) { 
    # Perfect linear relationship, so we can demonstrate with fewer data points.
    x = c(-10:10)/2 
  }  
    
  if (relationship == "positive") {
    # rho > 0
    y = 2*x+3
  } else {
    # relationship == "negative"
    # rho < 0
    y = -2*x+3
  }
  
  set.seed(seed)
  noise = rnorm(length(x),0,noise_sd)
  z = y + noise
  rho_rnd = round(cor(x,z), digits = 2)
  # print(cor(x,z))
  plot(x, z, main=bquote(~ rho == .(rho_rnd)), 
       xlab="", ylab="", axes=FALSE, frame.plot=TRUE)
  Axis(side=1, labels=FALSE)
  Axis(side=2, labels=FALSE)
  
  # return(TRUE) # don't return anything from the function
}

# linear_corr_gen(noise_sd=3, "positive", 23) # example usage

# linear_corr_gen(noise_sd=-3, "positive", 23) # this generates a warning

```


```{r graph-comments, include=FALSE}
# In .Rmd, we don't need to do par(mfrow=c(1,1)) at the end of a plot to restore to defaults.
# This is required only in the RStudio console.

# Given R UTF8 encoding issues in Windows,
# we cannot display ">=" or "<=" in the RMarkdown correctly. 
# Don't waste time to try this again!
```


```{r correlation-comments, include=FALSE}
# Consider writing a function to generate such graphs
# Input: positive vs negative, noise_sd

# Fixed x = c(-10:10)/2
# If rho > 0 => y = 2*x+3
# Else (rho < 0) => y = -2*x+3

# noise = rnorm(length(x),0,noise_sd)
# z = y + noise
# The higher noise_sd is, the weaker correlation between x and z,
# i.e., the closer |rho(x,z)| is to zero.

# Formulas: 
# corr(x,z) = cov(x,z)/sqrt(var(x)*var(z))
# y = ax + b
# var(y) = a^2 * var(x)
```

Our first set of graphs are created in Figure \ref{fig:perfect-linear-correlation}. These show what $|\rho| = 1$ looks like -- a straight line, i.e., a perfect linear relationship. In mathematical terms, we can write $Y = \alpha + \beta X$, where the sign of $\beta$ controls the direction of the straight line.  

- For $\rho = 1$, the two variables $X, Y$ have a perfect positive linear relationship ($\beta > 0$). One example is people's height in cm and height in inches, where the two variables are a positive linear combination of each other.     

- For $\rho = -1$, the two variables $X, Y$ have a perfect negative linear relationship ($\beta < 0$). One example is you eat from a box of 12 cookies. The cookies you ate and the cookies remaining should add up to 12, so they have a perfect negative linear relationship.   

```{r perfect-linear-correlation, fig.height=2.5, fig.width=5, fig.cap="Perfect linear correlation: $\\rho = 1$ (left) and $\\rho = -1$ (right)", echo=FALSE}

par(mfrow=c(1,2), mar = c(1.1, 1.1, 1.1, 1.1))

# rho = 1
linear_corr_gen(noise_sd=0, "positive", 36)

# rho = -1
linear_corr_gen(noise_sd=0, "negative", 37)
```

For the second set of graphs in Figure \ref{fig:high-correlation}, the two variables in each plot have a good but imperfect linear relationship. The datapoints seem to be on a straight line with some fluctuation. If we start with the linear equation, we can add a small random noise with mean zero to the data generative process. Hence the equation becomes $Y = \alpha + \beta X + \epsilon$, where $\epsilon \sim N(0,\sigma^2)$. Here we assume the random noise $\epsilon$ follows a normal distribution.       

- The left graph illustrates $\rho = 0.89$. The plot shows a clear and positive relationship with some noise. One example is the height and weight of children of various ages. Taller children usually weigh more, but this is not always the case. (Childhood obesity is a serious health problem.)        

- The right graph illustrates $\rho = -0.93$. The plot shows a clear and negative relationship with some noise. One example is the hours for work and the hours for hobbies. Generally, the more time you spend on work, the less time you spend on hobbies. Although everyone has 24 hours in a day, the time outside work does not always mean you are spending on hobbies for enjoyment.   

```{r high-correlation, fig.height=2.5, fig.width=5, fig.cap="Good but imperfect linear correlation: $\\rho = 0.89$ (left) and $\\rho = -0.93$ (right)", echo=FALSE}

par(mfrow=c(1,2), mar = c(1.1, 1.1, 1.1, 1.1))

# rho = 0.89
linear_corr_gen(noise_sd=3, "positive", 38)

# rho = -0.93
linear_corr_gen(noise_sd=2.5, "negative", 39)
```

The third set of graphs in Figure \ref{fig:mid-correlation} visualizes a medium level of linear correlation, where $0.4 \leq |\rho| < 0.6$. The linear trend is still visible, but with more fluctuation than in the high-correlation graphs.    

- The left graph shows $\rho = 0.64$, as a positive medium correlation of the two variables. One example is students' midterm exam scores and final exam scores. Students who did well on the midterm often continue to do well on the final, but this is not guaranteed. Exam scores often have more variability than people's height and weight.        

- The right graph shows $\rho = -0.47$, as a negative medium correlation of the two variables. One example is adults' muscle mass and their age. Older adults tend to have less muscle than younger ones, but it is well-known that strength training helps preserve some muscle mass.^[We are not medical professionals, so we make the caveat that the linear relationship may not be accurate.]     

```{r mid-correlation, fig.height=2.5, fig.width=5, fig.cap="Medium level of linear correlation: $\\rho = 0.89$ (left) and $\\rho = -0.93$ (right)", echo=FALSE}

par(mfrow=c(1,2), mar = c(1.1, 1.1, 1.1, 1.1))

# rho = 0.64
linear_corr_gen(noise_sd=7, "positive", 23)

# rho = -0.47
linear_corr_gen(noise_sd=12, "negative", 40)
```

The final set of graphs in Figure \ref{fig:low-correlation} illustrates what little-to-no linear correlation looks like. We need to remember that zero correlation does not mean the two variables are independent. They may have a non-linear relationship, which cannot be detected by the correlation coefficient.       

- The left graph shows a bivariate scatter plot with $\rho = 0.03$, i.e., very little correlation. For a small $\rho$ in absolute value, the two variables barely have any linear relationship. Although the bivariate scatter plot looks random, the correlation coefficient may not be exactly zero. One example is the daily coffee consumption and intelligence (IQ scores). How much coffee an individual drinks is completely unrelated to their intelligence level, so we cannot use one variable to obtain information of the other.^[<https://www.statology.org/no-correlation-examples/>]     

- The right graph has $\rho = 0$ (absolute zero), i.e., no linear relationship at all. However, the graph shows a parabola curve, so the two variables have some non-linear relationship. If you throw a ball to another person, the ball's trajectory would look like a parabola.^[<https://bit.ly/4dF3y4Z>] The x-axis is horizontal and parallel to the ground, while the y-axis is vertical and perpendicular to the x-axis.   


```{r low-correlation, fig.height=2.5, fig.width=5, fig.cap="Little-to-no linear correlation: $\\rho = 0.03$ (left) and $\\rho = 0$ (absolute zero) (right)", echo=FALSE}

par(mfrow=c(1,2), mar = c(1.1, 1.1, 1.1, 1.1))

# rho = 0.03
set.seed(1000)
x = runif(100,-10,10)
y = runif(100,-10,10)
rho_current = round(cor(x,y), digits = 2)
plot(x,y,main=bquote(~ rho == .(rho_current)), 
     xlab="", ylab="", axes=FALSE, frame.plot=TRUE)
Axis(side=1, labels=FALSE)
Axis(side=2, labels=FALSE)

# rho = 0
x = c(-10:10)/2

y = -x^2 + 25
rho_current = round(cor(x,y), digits = 2)
# plot(x,y, main=expression(paste(rho," = 0")))
plot(x,y,main=bquote(~ rho == .(rho_current)), 
     xlab="", ylab="", axes=FALSE, frame.plot=TRUE)
Axis(side=1, labels=FALSE)
Axis(side=2, labels=FALSE)
```

