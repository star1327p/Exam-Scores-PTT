In **k-fold cross validation**, we randomly divide the data into $k$ subsets to cross-validate each other. (Typically $k=10$.) For each round of validation, we train the model on the $k-1$ subsets and test the model on the one subset which was excluded in the training. Finally, we combine all $k$ rounds of validation, and each subset gets its predicted result for performance evaluation.   

To implement 10-fold cross validation, we divide the data into 10 partitions of near-equal sizes. We have 188 records, so there should be eight partitions of 19 records and two partitions of 18 records. We store the indices of each partition in the `partition_list`. 

```{r k-fold-partition-gen}
# 10-fold cross validation: 
# Divide 188 records into 10 partitions of near-equal size

# Number of records in each partition:
# 19, 19, 19, 19, 19, 19, 19, 19, 18, 18
k_fold = c(19, 19, 19, 19, 19, 19, 19, 19, 18, 18)
k_accumulate = c(19, 38, 57, 76, 95, 114, 133, 152, 170, 188)

k_fold_partition <- function(data, k_fold, seed) {
  # Generate the 10 partitions for the data
  
  set.seed(seed) 
  nn = nrow(data) # total 188 rows of data
  row_inds = c(1:nn)
  ind_permute = sample(row_inds) 
  # random permutation of row indices 
  # => prepare for the training/testing partitions
  
  partition_list = list(0,0,0,0,0,0,0,0,0,0)
  
  # Need to sort the indices within each partition
  partition_list[[1]] = sort(ind_permute[1:k_fold[1]])
  for (ii in 2:length(k_fold)) {
    start = k_accumulate[ii-1]+1
    end = start + k_fold[ii] - 1
    partition_list[[ii]] = sort(ind_permute[start:end])
  }
  
  return(partition_list)
}

partition_list = k_fold_partition(data_corr, k_fold, seed=21)
partition_list
```

After obtaining the 10 partitions, we reserve one partition as the testing set and feed the other 9 partitions into the training. Each partition gets the chance to be the testing set, and we obtain all 10 confusion matrices.  

Other `R` packages may have functions to handle k-fold cross validation, but we decided to write our own code to show the readers how the method is implemented from scratch. Moreover, this allows maximum flexibility for us to modify the code for future needs.

```{r k-fold-train-test}
k_fold_train_test <- function(data, partition_list, k_fold) {
  # Training and testing process for k-fold cross validation
  
  # Use the partitions for training and testing
  partition_probs = list(0,0,0,0,0,0,0,0,0,0)
  partition_matrices = list(0,0,0,0,0,0,0,0,0,0)
  
  for (exclude in 1:length(k_fold)) {
    # Testing parts
    testing_with_k = partition_list[[exclude]]
    test_kfold_data = data_corr[testing_with_k,]
    
    # Training parts
    # partition_list[-exclude] shows all elements except the exclude.
    training_without_k = unlist(partition_list[-exclude]) 
    # integer vector of training indices
    train_kfold_data = data_corr[training_without_k,]
    
    train_kfold_model = glm(CS_65up ~ HighSchool_PR, 
                            data=train_kfold_data, family="binomial")
    # summary(train_kfold_model)
    test_kfold_prob = predict.glm(train_kfold_model, 
                                  test_kfold_data, type="response")
    # type="response" gives the predicted probabilities
    
    # Store the predicted probabilities of each partition in a list
    partition_probs[[exclude]] = test_kfold_prob
    
    # Store the confusion matrix of each partition in another list
    partition_matrices[[exclude]] = prob_to_matrix(test_kfold_data, test_kfold_prob)
  }
  
  # partition_probs
  return(partition_matrices)
}

partition_matrices = k_fold_train_test(data_corr, partition_list, k_fold)
partition_matrices
```

Now we summarize the results in k-fold cross-validation, i.e., combine all 10 confusion matrices into one. We write a function to encapsulate the sum-up process of the confusion matrices, in order to reuse the code later.

```{r k-fold-comments, include=FALSE}
# Cannot directly add the partition matrices together.
# sum(partition_matrices[[1]] + partition_matrices[[2]]) # 38

# Unlist does not sort the indices, so this does not work, either.
# prob_to_matrix(data_corr, unlist(partition_probs))
```

```{r k-fold-summary}
sum_up_confusion <- function(k_fold, partition_matrices) {
  # Sum up all k confusion matrices into a single one.
  tp = 0
  fp = 0
  fn = 0
  tn = 0
  
  for (part in 1:length(k_fold)) {
    tp = tp + partition_matrices[[part]][1]
    fp = fp + partition_matrices[[part]][2]
    fn = fn + partition_matrices[[part]][3]
    tn = tn + partition_matrices[[part]][4]
  }
  
  # Use an existing confusion matrix as a template.
  k_fold_table = partition_matrices[[1]]
  
  k_fold_table[1,1] = tp
  k_fold_table[1,2] = fn
  k_fold_table[2,1] = fp
  k_fold_table[2,2] = tn
  
  return(k_fold_table)
} 

k_fold_table = sum_up_confusion(k_fold, partition_matrices)
k_fold_table
```

After obtaining the 10-fold cross validation results in a single confusion matrix, we can calculate the accuracy, precision, recall, FPR, FNR using the `confusion_to_measures` function from the previous section.

```{r confusion-to-measures}
k_fold_results = confusion_to_measures(k_fold_table)
round(k_fold_results, digits=4)
```

Since 10-fold cross validation involves randomly partitioning the data into 10 parts of nearly equal size, we can try different random seeds to see how the results change. For each of the five random seeds, the confusion matrices are quite similar.  

```{r k-fold-multiple-times}
set.seed(37)
runs = 5
# Discrete uniform distribution:
# Generate a sequence of random numbers between 1 and 1000
# (sample without replacement)
seed_each = sample(1:1000, runs, replace=F)

for (iter in 1:runs){
  partition_list = k_fold_partition(data_corr, k_fold,seed=seed_each[iter])
  partition_matrices = k_fold_train_test(data_corr, partition_list, k_fold)
  out_matrices[[iter]] = sum_up_confusion(k_fold, partition_matrices)
  print(out_matrices[[iter]])
}
```

We also output the results of the five iterations. 

```{r k-fold-multiple-tests}
out_measures = combine_results(out_matrices)

# out_measures
round(out_measures, digits=4)
```

Then we calculate the average of the five iterations, and the mean accuracy is slightly over 70%. Note that the actual numbers can vary depending on the random seed.

```{r k-fold-multiple-avg}
average = calc_average(out_measures)

# average
round(average, digits=4)
```