We can create a demonstrative ROC plot with the `roc.plot` function in the `R` package `verification`. We are using data that consist of actual binary outcomes (0/1), along with the predictive probabilities of each datapoint to be 1. Then we will explore different features of the `roc.plot` function, and explain how to interpret the results.   

### Simulated Data 

Let's generate some random data as a hypothetical example, where the first 500 observations are 0 and the remaining 500 observations are 1. The vector array `obs_outcomes` is the actual observed binary outcomes. This array consists of elements only 0's and 1's.      

```{r auc-simulated-data, warning=FALSE, message=FALSE}
library(verification)
group_size = 500
obs_outcomes = c(rep(0,group_size),rep(1,group_size))
```

We also assume a model: For an observed 0, the model predicts it as average 25% chance to be 1. For an observed 1, the model predicts it as average 75% chance to be 1. Let's generate the (fictitious) predicted probabilities using some statistical distributions. 

- For an observed 0, we would like the predicted probabilities to be centered at 0.25, so we start with a normal distribution with mean 0.25 and standard deviation 0.3.  

- For an observed 1, we would like the predicted probabilities to be centered at 0.75, so we start with a normal distribution with mean 0.75 and standard deviation 0.3.  

However, a normal distribution may generate values outside the range of [0,1], so we add a uniform distribution to offset this issue. This adjustment ensures all predicted probabilities are valid, i.e., between 0 and 1.     

- When the normal distribution outputs a value less than 0, we should replace the value with another value generated by the uniform distribution between 0 and 0.01.

- When the normal distribution outputs a value larger than 1, we should replace the value with another value generated by the uniform distribution between 0.99 and 1.   

Now let's generate the predictive probabilities for the actual 0 and 1 datapoints, and we store them in the vector arrays `pred_prob_0` and `pred_prob_1`, respectively.  

```{r auc-demo-gen, warning=FALSE, message=FALSE}
set.seed(3333)
pred_prob_0 = rnorm(group_size, mean=0.25,sd=0.3)
# Ensure all probability values are greater or equal to 0.  
pred_prob_0 = pmax(pred_prob_0, runif(group_size,min=0,max=0.01))
# Ensure all probability values are less than or equal to 1.
pred_prob_0 = pmin(pred_prob_0, runif(group_size,min=0.99,max=1))
                                      
pred_prob_1 = rnorm(group_size, mean=0.75,sd=0.3)
# Ensure all probability values are greater or equal to 0.  
pred_prob_1 = pmax(pred_prob_1, runif(group_size,min=0,max=0.01))
# Ensure all probability values are less than or equal to 1.
pred_prob_1 = pmin(pred_prob_1, runif(group_size,min=0.99,max=1))
```

In the code above, we use `set.seed` to specify a random seed to ensure reproducibility of outcomes.   

- The functions starting with "r" (random) and a distribution name generates random values from the distribution given its parameters. For example, `rnorm` generates random values from a normal distribution, given its mean and standard deviation. Similarly, `runif` generates random values from a continuous uniform distribution, given its lower bound and upper bound. Note that `runif` is pronounced as "r-unif" instead of "run-if".  

- In the functions `pmin` and `pmax`, the first letter "p" stands for "parallel" computation. Each function takes an input of two numerical vectors of the same length, and returns a single vector with the "parallel" minima (or maxima) of the input vectors. In other words, we compare the two input vectors to find the minimum (or maximum) for each position.  

Under the scheme of combining the normal distribution and the uniform distribution, we can check that all output values are valid probabilities (i.e., between 0 and 1).  

```{r auc-demo-check}
number_of_valid_prob_0 = sum((pred_prob_0 >= 0) & (pred_prob_0 <= 1))
number_of_valid_prob_1 = sum((pred_prob_1 >= 0) & (pred_prob_1 <= 1))

print(paste("Among all",group_size,"values in pred_prob_0,",
            number_of_valid_prob_0, "of them are valid probabilities."))

print(paste("Among all",group_size,"values in pred_prob_1,",
            number_of_valid_prob_1, "of them are valid probabilities."))
```

### The `roc.plot` Function {#roc-plot}

Now let's make the demonstrative ROC curve using `roc.plot`, which plots the false positive rate (FPR) against the false negative rate (FNR). The first input vector is the actual observed outcomes `obs_outcomes`, and the second input vector is the predicted probabilities `pred`. The curve should be above the straight line from the bottom left (0,0) to the top right (1,1). We also store the output of `roc.plot` in a new object `new_test` for future use, which is of `roc.data` type.   

```{r auc-demo-roc}
pred = c(pred_prob_0, pred_prob_1)
new_test = roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE,
                    main="ROC Curve (Simulated Data)")
```

We can access the model information in the `new_test` object with the `roc.vol` label. 

```{r auc-demo-roc-vol}
print(new_test$roc.vol)
```

- The `Area` is the AUC (area under the curve), which is approximately 0.89. Typically we need AUC at least 0.7 for the model performance to be considered adequate. The straight line is the baseline as AUC = 0.5.
- The `p.value` is calculated against the null hypothesis $H_0$ as random guess (AUC $\approx$ 0.5). In this example, the p-value is extremely small, showing statistically significant evidence to reject $H_0$.
- The `binorm.area` is `NA` (not available). This applies only if we select a binormal fit.  

We can also directly retrieve the AUC value.

```{r auc-demo-retrieve}
auc_value = new_test$roc.vol$Area
print(auc_value)
```

Let's add the AUC value to the title of the ROC graph.

```{r auc-demo-continued}
roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = FALSE,
         main=paste("ROC Curve: AUC =", round(auc_value,2), "(Simulated Data)"))
```


### More Details  

The ROC graph is a summary of the model performance across all probability thresholds, and each point on the curve represents the FPR (false positive rate) and TPR (true positive rate) of a single probability threshold. When we set `show.thres = TRUE` in the `roc.plot` function, the graph shows that the probability thresholds range from 0 to 1.       

```{r auc-demo-threshold}
roc.plot(obs_outcomes,pred,xlab = "FPR", ylab="TPR",show.thres = TRUE,
         main="ROC Curve with Probability Threshold for Classification")
```

- At the left of the curve, a high probability threshold such as 0.9 sets a high criteria for the model to predict a datapoint as positive. This results in a low FPR , because when the datapoint is actually negative, the model is extremely unlikely to predict it as positive. But on the other hand, the high probability threshold also makes it relatively difficult to predict a datapoint to be positive. Hence the TPR is low because the actual positive datapoints may or may not receive a positive prediction.  

- At the right of the curve, a low probability threshold such as 0.1 sets a low criteria for the model to predict a datapoint as positive. This results in a high TPR, because when the datapoint is actually positive, the model is extremely likely to predict it as positive. But on the other hand, the low probability threshold also predicts too many datapoints to be positive, and some of them are actually negative. Hence the FPR is also high.   

The underlying data for the ROC graph is stored in the matrix `new_test$plot.data`. The first column (`V1`) is the probability threshold used to convert predictive probabilities into binary predictions. The second column (`V2`) is the empirical hit rate (true positive rate, TPR), and the third column (`V3`) is the false alarm rate (false positive rate, FPR).    

We can manually plot the FPR against the TPR, but this is NOT recommended. We should use a pre-built function whenever possible, because these functions have been optimized for performance. The ROC graph we created is not as clean as the one generated by the function `roc.plot` in Section \ref{roc-plot}.       

```{r auc-demo-roc-data-comments, include=FALSE}
# test_df columns:
# V1 = threshold
# V2 = empirical hit rate (TPR)
# V3 = false alarm rate (FPR)
```

```{r auc-demo-roc-data}
test_df = as.data.frame(new_test$plot.data)
plot(test_df$V3, test_df$V2, xlab="FPR", ylab="TPR",
     main = "ROC Curve: Manually Created") 

# add the diagonal line (a = intercept, b = slope)
abline(a=0, b=1) 
# add the grid lines in gray
# nx and ny are the number of cells of the grid in x and y direction.
grid(nx = 10, ny = 10, col = "lightgray", lty = "dotted")
```