This manuscript is a detailed example of a project from data collection to meaningful insights, and we assume a background equivalent to Statistics 101. In the end-to-end execution, we also demonstrated good coding practices such as user-defined functions, descriptive variable names, and helpful comments in code. These practices in `R` are transferable to other programming languages like Python and Java, which are commonly-used in the tech industry.    

We obtained small-scale data that are publicly available and articulated our thought process in each step. We started with data preprocessing (i.e., cleaning the data), explored the data, and applied statistical models to quantify the relationship between the two variables of interest. The first model we try does not always work out, and the model validation can reveal more data issues previously undiscovered in the exploratory phase. Given the new findings, we can try different ways to continue the analysis.  

Our original problem is to find the relationship between high school entrance exam scores and college entrance exam scores. The exploratory analysis in Chapter \ref{eda} shows a moderate positive association between the two sets of scores, with the correlation coefficient 0.50. However, this does not necessarily mean that the linear regression is the most appropriate model. Linear regression is widely used, but it is not a panacea for data analysis. Chapter \ref{linear-reg} states the four assumptions to be met in linear regression: linearity, nearly normal residuals, constant variability, and independent observations. When these assumptions are violated, we should not use the linear regression model.  

Although the linear regression did not work out, we took a deep dive to understand the data breakdown by high school entrance exam score category in Chapter \ref{explore-top}. It is possible that a model has different levels of performance in different ranges of the independent variable. Exploratory data analysis shows the data are left-skewed, i.e., much more respondents with high scores than low scores. This chapter is focused on the top scorers because they account for the majority of the respondents.  

The good news is that we can continue the data analysis through several ways:

- Transform the data to meet the assumption requirements

- Choose another model to investigate the relationship between the two variables

- Reformulate the problem and answer a different question from the data

In Chapter \ref{logit-reg}, we reformulated the problem to: Given a student's high school entrance exam score, estimate the probability of getting a college entrance exam score of at least 65. This statement binarizes the outcome of college entrance exam scores, so we discovered the relationship of the two variables in a different way. Sections \ref{logit-point-est} and \ref{logit-results} show that when the high school entrance exam score increases by one percentile, the odds of "success" increases by about 16.1% on average, i.e., getting a college entrance exam score at least 65. We are 95% confident  that the odds increase is between 10.6% and 22.9%.  

After implementing the logistic regression model, we also validated the results using in-sample prediction (Chapter \ref{validation}) and out-of-sample prediction (Chapter \ref{out-of-sample}). The accuracy is approximately 70% and consistent across both methods of prediction. About half of the respondents obtained a college entrance exam score at least 65, so the baseline is around 50% like a coin-flip. Hence the logistic regression model is doing much better compared with the baseline, and the detailed metrics are listed and explained in Section \ref{cmp-results}.  

We also examined the model results by high school entrance exam score category in Section \ref{another-breakdown}. Since this is a binary classification model, we focused on the confusion matrices of each category. For the respondents whose high school entrance exam score are in the 0-79th percentile, the model predicted none of them to obtain a college entrance exam score of at least 65, but some of these respondents actually did succeed. For the respondents in the 80-89th percentile, the predictions are the same but with a higher proportion of unexpected successes. The model predictions consist of both success and non-success outcomes for the respondents in the 90-94th percentile. Finally, the model predicts all respondents in the 95-99th percentile to obtain a college entrance exam score of at least 65, and inevitably some of them did not achieve this. Given the single outcome prediction in most categories, we added zero columns/rows to ensure the 2x2 size of each confusion matrix.  